{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KT sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import silnlp.common.paratext\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from silnlp.common.corpus import load_corpus\n",
    "\n",
    "# paths for generic Major KT lists\n",
    "metadata_path = Path(\"silnlp/assets/Major-metadata.txt\")\n",
    "vrefs_path = Path(\"silnlp/assets/Major-vrefs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_gloss_path = Path(\"silnlp/assets/fr-Major-glosses.txt\") # en  fr\n",
    "trg_gloss_path = Path(\"test_S/MT/terms/bcw-bcw_2024_02_21-Major-renderings.txt\") # lmp-lmp_2024_02_16  bcw-bcw_2024_02_21\n",
    "pair = \"fr_bcw\" # en_lmp  fr_bcw\n",
    "\n",
    "proper_nouns = defaultdict(dict)\n",
    "for i, (meta, vref, src_gloss, trg_gloss) in enumerate(zip(load_corpus(metadata_path), load_corpus(vrefs_path), load_corpus(src_gloss_path), load_corpus(trg_gloss_path))):\n",
    "    term, pt_cat, sem_cat = meta.split(\"\\t\") # orig lang term, Paratext category (PN, FL, RE, etc.), semantic category (person, grasses, containers, etc.)\n",
    "    instances = vref.split(\"\\t\") # all occurrences of the term\n",
    "    src_glosses = src_gloss.split(\"\\t\") # all potential glosses for term\n",
    "    trg_glosses = trg_gloss.split(\"\\t\")\n",
    "\n",
    "    if pt_cat == \"PN\" and trg_glosses != [\"\"]:\n",
    "        proper_nouns[i][\"glosses\"] = (src_glosses, trg_glosses)\n",
    "        proper_nouns[i][\"instances\"] = instances # might want to give this further structure, i.e. be a dict w/ book:chapter:[instances]\n",
    "\n",
    "with open(f\"zzz_PN_KTs/{pair}/KT_to_vrefs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(proper_nouns, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Create verse-to-KTs dict\n",
    "vref_to_KTs = defaultdict(list)\n",
    "for i, pn_dict in proper_nouns.items():\n",
    "    for vref in pn_dict[\"instances\"]:\n",
    "        vref_to_KTs[vref].append(i)\n",
    "with open(f\"zzz_PN_KTs/{pair}/vref_to_KTs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vref_to_KTs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix KTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silnlp.common.corpus import load_corpus\n",
    "from pathlib import Path\n",
    "from machine.corpora import ScriptureRef\n",
    "from silnlp.alignment.utils import compute_alignment_scores\n",
    "\n",
    "pair = \"\"\n",
    "book_name = \"08RUT\"\n",
    "vrefs = [ScriptureRef.parse(ref) for ref in load_corpus(Path(f\"zzz_PN_KTs/{pair}/{book_name}_vrefs.txt\"))]\n",
    "src_path = Path(f\"zzz_PN_KTs/{pair}/{book_name}_src_sents.txt\")\n",
    "trg_path = Path(f\"zzz_PN_KTs/{pair}/{book_name}_trg_sents.txt\")\n",
    "\n",
    "# always uses LatinWordTokenizer\n",
    "sym_align_path = Path(f\"zzz_PN_KTs/{pair}/{book_name}_sym-align.txt\")\n",
    "scores = compute_alignment_scores(src_path, trg_path, aligner_id=\"eflomal\", sym_align_path=sym_align_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.tokenization import LatinWordTokenizer\n",
    "from machine.corpora import TextFileTextCorpus\n",
    "from machine.scripture import VerseRef\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "# treat_apostrophe_as_single_quote=True didn't do anything\n",
    "# confirmed that these have the same tokenization as the aligner (for Latin script)\n",
    "# aligner uses LatinWordTokenizer + escape_spaces, nfc_normalize, lowercase from TextCorpus\n",
    "src_lines = [line.segment for line in TextFileTextCorpus(src_path).tokenize(LatinWordTokenizer()).lowercase()]\n",
    "trg_lines = [line.segment for line in TextFileTextCorpus(trg_path).tokenize(LatinWordTokenizer())]\n",
    "src_lines_raw = load_corpus(src_path)\n",
    "trg_lines_raw = load_corpus(trg_path)\n",
    "\n",
    "align_lines = [[(lambda x: (int(x[0]), int(x[1])))(pair.split(\":\")[0].split(\"-\")) for pair in line.split()] for line in load_corpus(sym_align_path)]\n",
    "\n",
    "# # check for alignment coverage\n",
    "# # not complete coverage, so can't assume anything about if specific words are aligned\n",
    "# for i, (ref,src_line,trg_line,align_pairs) in enumerate(zip(vrefs, src_lines, trg_lines, align_lines)):\n",
    "#     src_idxs = {pair[0] for pair in align_pairs}\n",
    "#     trg_idxs = {pair[1] for pair in align_pairs}\n",
    "\n",
    "#     print(i+1, ref)\n",
    "#     print(f\"unaligned SRC: {len(src_line) - len(src_idxs)}\")\n",
    "#     print(f\"unaligned TRG: {len(trg_line) - len(trg_idxs)}\")\n",
    "\n",
    "pair = \"en_lmp\"\n",
    "book = \"RUT\"\n",
    "with open(f\"zzz_PN_KTs/{pair}/vref_to_KTs.json\", encoding=\"utf-8\") as f:\n",
    "    vref_to_KTs = json.load(f)\n",
    "with open(f\"zzz_PN_KTs/{pair}/KT_to_vrefs.json\", encoding=\"utf-8\") as f:\n",
    "    KT_to_vrefs = json.load(f)\n",
    "\n",
    "term_ids = set()\n",
    "exp_vrefs = set()\n",
    "for ref, ids in vref_to_KTs.items():\n",
    "    if VerseRef.from_string(ref).book == book:\n",
    "        term_ids.update(ids)\n",
    "        exp_vrefs.add(ref)\n",
    "src_terms = set()\n",
    "trg_terms = set()\n",
    "for id in term_ids:\n",
    "    src_terms.update(KT_to_vrefs[str(id)][\"glosses\"][0])\n",
    "    trg_terms.update(KT_to_vrefs[str(id)][\"glosses\"][1])\n",
    "print(src_terms)\n",
    "\n",
    "# found = defaultdict(list)\n",
    "for ref,src_line,trg_line,align_pairs,trg_line_raw in zip(vrefs, src_lines, trg_lines, align_lines,trg_lines_raw):\n",
    "    if str(ref.verse_ref) not in vref_to_KTs.keys():\n",
    "        continue\n",
    "    if ref.verse_num == 0 or ref.path[0].name != \"\": # the ScriptureRefs I'm testing with have an empty ScriptureElement in the path so is_verse doesn't work\n",
    "        continue\n",
    "\n",
    "    found = []\n",
    "    for term_id in vref_to_KTs[str(ref.verse_ref)]:\n",
    "        glosses = [gloss.lower() for gloss in KT_to_vrefs[str(term_id)][\"glosses\"][0]]\n",
    "        min_dist = (0, 0, 100) # gloss idx of closest match, tok idx of closest match, distance\n",
    "        for i, gloss in enumerate(glosses): # could adjust this to look at n-grams, where n is the number of words in the gloss\n",
    "            for j, tok in enumerate(src_line):\n",
    "                if (j, term_id) in found:\n",
    "                    continue\n",
    "                dist = nltk.edit_distance(gloss, tok) / len(tok)\n",
    "                if dist < min_dist[2]:\n",
    "                    min_dist = (i, j, dist)\n",
    "        # print(glosses[min_dist[0]], src_line[min_dist[1]], min_dist[2])\n",
    "        if min_dist[2] < .3:\n",
    "            found.append((min_dist[1], term_id))\n",
    "    # print(ref)\n",
    "    # print(len(found), len(vref_to_KTs[str(ref.verse_ref)]))\n",
    "    # print(found)\n",
    "    # print(vref_to_KTs[str(ref.verse_ref)])\n",
    "    # print(src_line)\n",
    "\n",
    "    # replace word(s) in target text\n",
    "    for src_idx, term_id in found:\n",
    "        trg_idxs = [pair[1] for pair in align_pairs if pair[0] == src_idx]\n",
    "        print(src_idx, trg_idxs)\n",
    "        print(src_line[src_idx], [trg_line[idx] for idx in trg_idxs])\n",
    "        print(trg_line_raw)\n",
    "        print(trg_line)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation -- no good\n",
    "* score orig --> no inline markers\n",
    "* score orig --> only para markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from pathlib import Path\n",
    "\n",
    "out = list(load_corpus(Path(\"\")))\n",
    "ref = [list(load_corpus(Path(\"\")))]\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(out, ref, lowercase=True).score\n",
    "spbleu = sacrebleu.corpus_bleu(out, ref, lowercase=True, tokenize=\"flores200\").score\n",
    "chrf = sacrebleu.corpus_chrf(out, ref, char_order=6, beta=3, remove_whitespace=True).score\n",
    "chrfp = sacrebleu.corpus_chrf(out, ref, char_order=6, beta=3, word_order=1, remove_whitespace=True, eps_smoothing=True).score\n",
    "chrfpp = sacrebleu.corpus_chrf(out, ref, char_order=6, beta=3, word_order=2, remove_whitespace=True, eps_smoothing=True).score\n",
    "print(bleu, spbleu, chrf, chrfp, chrfpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Goal Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import (\n",
    "    FileParatextProjectSettingsParser, \n",
    "    UsfmFileText, \n",
    "    UpdateUsfmParserHandler, \n",
    "    UsfmTokenizer, \n",
    "    UsfmTokenType, \n",
    "    parse_usfm, \n",
    "    UsfmParserState,\n",
    "    UpdateUsfmBehavior\n",
    "    )\n",
    "from pathlib import Path\n",
    "\n",
    "class ParagraphUpdateUsfmParserHandler(UpdateUsfmParserHandler):\n",
    "    def _collect_tokens(self, state: UsfmParserState) -> None:\n",
    "        self._tokens.extend(self._new_tokens)\n",
    "        self._new_tokens.clear()\n",
    "        while self._token_index <= state.index + state.special_token_count:\n",
    "            if state.tokens[self._token_index].type == UsfmTokenType.PARAGRAPH and state.tokens[self._token_index].marker != \"rem\":\n",
    "                num_text = 0\n",
    "                rem_offset = 0\n",
    "                for i in range(len(self._tokens) - 1, -1, -1):\n",
    "                    if self._tokens[i].type == UsfmTokenType.TEXT:\n",
    "                        num_text += 1\n",
    "                    elif self._tokens[i].type == UsfmTokenType.PARAGRAPH and self._tokens[i].marker == \"rem\":\n",
    "                        rem_offset += num_text + 1\n",
    "                        num_text = 0\n",
    "                    else:\n",
    "                        break\n",
    "                if num_text >= 2:\n",
    "                    self._tokens.insert(-(rem_offset + num_text - 1), state.tokens[self._token_index])\n",
    "                    self._token_index += 1\n",
    "                    break # should this be continue instead? what situations are there where \n",
    "            self._tokens.append(state.tokens[self._token_index])\n",
    "            self._token_index += 1\n",
    "\n",
    "pair = \"spa_zpu\"\n",
    "src_project = \"DHH94\"\n",
    "src_file_suffix = \"DHH94\"\n",
    "trg_project = \"zpuAT_2025_01_15\"\n",
    "trg_file_suffix = \"zpuAT\"\n",
    "\n",
    "book = \"PSA\"\n",
    "book_name = f\"19{book}\"\n",
    "trg_file_path = Path(f\"test_S/Paratext/projects/{trg_project}/{book_name}{trg_file_suffix}.SFM\")\n",
    "out_file_path = Path(f\"zzz_USFM/{pair}/{book}/{book_name}{trg_file_suffix}_goal.SFM\")\n",
    "trg_settings = FileParatextProjectSettingsParser(trg_file_path.parent).parse()\n",
    "trg_file_text = UsfmFileText(\n",
    "    trg_settings.stylesheet,\n",
    "    trg_settings.encoding,\n",
    "    trg_settings.get_book_id(trg_file_path.name),\n",
    "    trg_file_path,\n",
    "    trg_settings.versification,\n",
    "    include_markers=True,\n",
    "    include_all_text=True,\n",
    "    project=trg_settings.name,\n",
    ")\n",
    "\n",
    "tokenizer = UsfmTokenizer(trg_settings.stylesheet)\n",
    "sentence_toks = []\n",
    "vrefs = []\n",
    "for sent in trg_file_text:\n",
    "    toks = tokenizer.tokenize(sent.text.strip())\n",
    "    if len(toks) > 0:\n",
    "        sentence_toks.append(toks)\n",
    "        vrefs.append(sent.ref)\n",
    "\n",
    "to_delete = [\"fig\"]\n",
    "out_toks = []\n",
    "for i, (toks, ref) in enumerate(zip(sentence_toks, vrefs)):\n",
    "    out_toks.append([\"\"])\n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(toks):\n",
    "        if ignore_scope is not None:\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignore_scope = tok\n",
    "        elif tok.type == UsfmTokenType.PARAGRAPH:\n",
    "            out_toks[-1].append(\"\")\n",
    "        elif tok.type in [UsfmTokenType.TEXT, UsfmTokenType.CHARACTER, UsfmTokenType.END]:\n",
    "            out_toks[-1][-1] += tok.to_usfm()\n",
    "\n",
    "translated_rows = []\n",
    "for ref, sent in zip(vrefs, out_toks):\n",
    "    for segment in sent:\n",
    "        translated_rows.append((ref, segment))\n",
    "\n",
    "# Get note-type segments from src project\n",
    "src_file_path = Path(f\"test_S/Paratext/projects/{src_project}/{book_name}{src_file_suffix}.SFM\")\n",
    "src_settings = FileParatextProjectSettingsParser(src_file_path.parent).parse()\n",
    "src_file_text = UsfmFileText(\n",
    "    src_settings.stylesheet,\n",
    "    src_settings.encoding,\n",
    "    src_settings.get_book_id(src_file_path.name),\n",
    "    src_file_path,\n",
    "    src_settings.versification,\n",
    "    include_markers=True,\n",
    "    include_all_text=True,\n",
    "    project=src_settings.name,\n",
    ")\n",
    "tokenizer = UsfmTokenizer(src_settings.stylesheet)\n",
    "sentence_toks = [tokenizer.tokenize(sent.text.strip()) for sent in src_file_text]\n",
    "vrefs = [s.ref for s in src_file_text]\n",
    "ignored_segments = []\n",
    "for i, (toks, ref) in enumerate(zip(sentence_toks, vrefs)):\n",
    "    ignored_segment = \"\"\n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(toks):\n",
    "        if ignore_scope is not None:\n",
    "            ignored_segment += tok.to_usfm()\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignored_segments.append((ref, ignored_segment))\n",
    "                ignored_segment = \"\"\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignored_segment += tok.to_usfm()\n",
    "            ignore_scope = tok\n",
    "\n",
    "# Add any note-type segments back to the ends of their verses\n",
    "past = 0 # number of ignored segments added as their own row\n",
    "append = 0\n",
    "segment_idx = 0\n",
    "rows = []\n",
    "for i, (ref, row_text) in enumerate(translated_rows):\n",
    "    # insert into new row, is this possible for the real scenario, i.e. everything coming from the same project?\n",
    "    while segment_idx < len(ignored_segments) and ignored_segments[segment_idx][0] < ref:\n",
    "        rows.append(([ignored_segments[segment_idx][0]], ignored_segments[segment_idx][1]))\n",
    "        segment_idx += 1\n",
    "        past += 1\n",
    "    # if inserting into a current row, it should only happen in the last row for each ScriptureRef\n",
    "    if i == len(translated_rows) - 1 or translated_rows[i + 1][0] != ref:\n",
    "        while (segment_idx < len(ignored_segments) and ignored_segments[segment_idx][0] == ref):\n",
    "            row_text += ignored_segments[segment_idx][1]\n",
    "            segment_idx += 1\n",
    "            append += 1\n",
    "    rows.append(([ref], row_text))\n",
    "# add any remaining ignored segments\n",
    "for segment in ignored_segments[segment_idx:]:\n",
    "    rows.append(([segment[0]], segment[1]))\n",
    "print(past, append)\n",
    "\n",
    "'''Update file and write out'''\n",
    "with open(trg_file_path, encoding=trg_settings.encoding) as f:\n",
    "    usfm = f.read()\n",
    "handler = ParagraphUpdateUsfmParserHandler(rows, behavior=UpdateUsfmBehavior.PREFER_NEW)\n",
    "parse_usfm(usfm, handler, trg_settings.stylesheet, trg_settings.versification, preserve_whitespace=False)\n",
    "usfm_out = handler.get_usfm(trg_settings.stylesheet)\n",
    "with out_file_path.open(\"w\", encoding=trg_settings.encoding) as f:\n",
    "    f.write(usfm_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find vref differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "src_fpath = Path(\"test_S/Paratext/projects/DHH94/19PSADHH94.SFM\")\n",
    "trg_fpath = Path(\"test_S/Paratext/projects/zpuAT_2025_01_15/19PSAzpuAT.SFM\")\n",
    "src_out = Path(\"vrefs_src.txt\")\n",
    "trg_out = Path(\"vrefs_trg.txt\")\n",
    "\n",
    "# cp1252\n",
    "with src_fpath.open(encoding=\"utf-8-sig\") as f, src_out.open(\"w\") as out:\n",
    "    out.writelines([line.split(\" \")[0].strip() + \"\\n\" for line in f])\n",
    "with trg_fpath.open(encoding=\"utf-8-sig\") as f, trg_out.open(\"w\") as out:\n",
    "    out.writelines([line.split(\" \")[0].strip() + \"\\n\" for line in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find vref differences -- machine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from machine.corpora import FileParatextProjectSettingsParser, UsfmFileText\n",
    "\n",
    "src_fpath = Path(\"test_S/Paratext/projects/DHH94/19PSADHH94.SFM\")\n",
    "trg_fpath = Path(\"test_S/Paratext/projects/zpuAT_2025_01_15/19PSAzpuAT.SFM\")\n",
    "src_out = Path(\"vrefs_src.txt\")\n",
    "trg_out = Path(\"vrefs_trg.txt\")\n",
    "\n",
    "src_settings = FileParatextProjectSettingsParser(src_fpath.parent).parse()\n",
    "src_file_text = UsfmFileText(\n",
    "    src_settings.stylesheet,\n",
    "    src_settings.encoding,\n",
    "    src_settings.get_book_id(src_fpath.name),\n",
    "    src_fpath,\n",
    "    src_settings.versification,\n",
    "    include_markers=True,\n",
    "    include_all_text=True,\n",
    "    project=src_settings.name,\n",
    ")\n",
    "with src_out.open(\"w\") as f:\n",
    "    for sent in src_file_text:\n",
    "        f.write(f\"{sent.ref}\\n\")\n",
    "\n",
    "trg_settings = FileParatextProjectSettingsParser(trg_fpath.parent).parse()\n",
    "trg_file_text = UsfmFileText(\n",
    "    trg_settings.stylesheet,\n",
    "    trg_settings.encoding,\n",
    "    trg_settings.get_book_id(trg_fpath.name),\n",
    "    trg_fpath,\n",
    "    trg_settings.versification,\n",
    "    include_markers=True,\n",
    "    include_all_text=True,\n",
    "    project=trg_settings.name,\n",
    ")\n",
    "with trg_out.open(\"w\") as f:\n",
    "    for sent in trg_file_text:\n",
    "        f.write(f\"{sent.ref}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out all paragraph and character markers for a book\n",
    "To use, set book, fpath, and out_path. fpath should be a path to a book in a Paratext project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from machine.corpora import FileParatextProjectSettingsParser, UsfmFileText, UsfmTokenizer, UsfmTokenType\n",
    "\n",
    "# this assumes fpath is a book in a Paratext project folder\n",
    "src_fpath = Path(\"test_S/Paratext/projects/msSMBv0_2024_10_24/43LUKmsSMBv0.SFM\")\n",
    "trg_fpath = Path(\"test_S/Paratext/projects/NIrV/43LUKusNIRV14.SFM\")\n",
    "src_out = Path(\"markers_src.txt\")\n",
    "trg_out = Path(\"markers_trg.txt\")\n",
    "\n",
    "# file 1\n",
    "settings = FileParatextProjectSettingsParser(src_fpath.parent).parse()\n",
    "file_text = UsfmFileText(\n",
    "    settings.stylesheet,\n",
    "    settings.encoding,\n",
    "    \"\",\n",
    "    src_fpath,\n",
    "    settings.versification,\n",
    "    include_markers=True,\n",
    "    include_all_text=True,\n",
    "    project=settings.name,\n",
    ")\n",
    "\n",
    "to_delete = [\"fig\", \"va\", \"vp\"]\n",
    "vrefs = []\n",
    "usfm_markers = []\n",
    "usfm_tokenizer = UsfmTokenizer(settings.stylesheet)\n",
    "for sent in file_text:\n",
    "    if len(sent.ref.path) > 0 and sent.ref.path[-1].name == \"rem\":\n",
    "        continue\n",
    "\n",
    "    vrefs.append(sent.ref)\n",
    "    usfm_markers.append([])\n",
    "    usfm_toks = usfm_tokenizer.tokenize(sent.text.strip())\n",
    "    \n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(usfm_toks):\n",
    "        if ignore_scope is not None:\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignore_scope = tok\n",
    "        elif tok.type in [UsfmTokenType.PARAGRAPH, UsfmTokenType.CHARACTER, UsfmTokenType.END]:\n",
    "            usfm_markers[-1].append(tok.marker)\n",
    "\n",
    "with src_out.open(\"w\", encoding=settings.encoding) as f:\n",
    "    for ref, markers in zip(vrefs, usfm_markers):\n",
    "        f.write(f\"{ref} {markers}\\n\")\n",
    "\n",
    "# file 2\n",
    "settings = FileParatextProjectSettingsParser(trg_fpath.parent).parse()\n",
    "file_text = UsfmFileText(\n",
    "    settings.stylesheet,\n",
    "    settings.encoding,\n",
    "    \"\",\n",
    "    trg_fpath,\n",
    "    settings.versification,\n",
    "    include_markers=True,\n",
    "    include_all_text=True,\n",
    "    project=settings.name,\n",
    ")\n",
    "\n",
    "vrefs = []\n",
    "usfm_markers = []\n",
    "usfm_tokenizer = UsfmTokenizer(settings.stylesheet)\n",
    "for sent in file_text:\n",
    "    if len(sent.ref.path) > 0 and sent.ref.path[-1].name == \"rem\":\n",
    "        continue\n",
    "\n",
    "    vrefs.append(sent.ref)\n",
    "    usfm_markers.append([])\n",
    "    usfm_toks = usfm_tokenizer.tokenize(sent.text.strip())\n",
    "    \n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(usfm_toks):\n",
    "        if ignore_scope is not None:\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignore_scope = tok\n",
    "        elif tok.type in [UsfmTokenType.PARAGRAPH, UsfmTokenType.CHARACTER, UsfmTokenType.END]:\n",
    "            usfm_markers[-1].append(tok.marker)\n",
    "\n",
    "with trg_out.open(\"w\", encoding=settings.encoding) as f:\n",
    "    for ref, markers in zip(vrefs, usfm_markers):\n",
    "        f.write(f\"{ref} {markers}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make table of style/text types of usfm markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tnone\tCharacter\tNote\tParagraph\tEnd\tMilestone\tMilestoneEnd\n",
      "none\t\tv\t\timt2,c\t\t\t\n",
      "Title\t\t\t\tmt,mt1,mt2,mt3,mt4,mte,mte1,mte2\t\t\t\n",
      "Section\t\t\t\tms,ms1,ms2,ms3,mr,s,s1,s2,s3,s4,sr,r,sp,sd,sd1,sd2,sd3,sd4,periph\t\t\t\n",
      "VerseText\tb,li,li1,li2,li3,li4,lim,lim1,lim2,lim3,lim4,pub,toc,pref,intro,conc,glo,idx,maps,cov,spine\tqs,th1,th2,th3,th4,th5,tc1,tc2,tc3,tc4,tc5,thc1,thc2,thc3,thc4,thc5,tcc1,tcc2,tcc3,tcc4,tcc5,thr1,thr2,thr3,thr4,thr5,tcr1,tcr2,tcr3,tcr4,tcr5,litl,lik,liv,liv1,liv2,liv3,liv4,liv5,qt,nd,tl,dc,bk,sig,pn,png,addpn,wj,k,sls,ord,add,rb,w,wh,wg,wa,ndx,wr\t\tp,m,po,pr,cls,pmo,pm,pmc,pmr,pi,pi1,pi2,pi3,pc,mi,nb,q,q1,q2,q3,q4,qc,qr,qm,qm1,qm2,qm3,qd,d,tr,lh,lf,ph,ph1,ph2,ph3,phi,tr1,tr2,ps,psi\t\t\t\n",
      "NoteText\t\tfr,ft,fk,fq,fqa,fl,fw,fp,fv,fdc,xo,xop,xt,xta,xk,xq,xot,xnt,xdc,xtSee,fs\tf,fe,x\t\t\t\t\n",
      "Other\tio,io1,io2,io3,io4,ili,ili1,ili2,ib,ie\tior,iqt,ca,va,vp,qac,fm,rq,no,it,bd,bdit,em,sc,sup,fig,jmp,pro,xtSeeAlso,zpa-xb,zpa-xc,zpa-xv\t\tid,usfm,ide,h,h1,h2,h3,toc1,toc2,toc3,toca1,toca2,toca3,rem,sts,restore,imt,imt1,imt3,imt4,imte,imte1,imte2,is,is1,is2,iot,ip,im,ipi,imi,ipq,imq,ipr,iq,iq1,iq2,iq3,iex,cp,cl,cd,qa,lit,pb,p1,p2,k1,k2,pubinfo\t\t\t\n",
      "BackTranslation\t\t\t\t\t\t\t\n",
      "TranslationNote\t\t\t\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "# not covered: ChapterNumber\n",
    "# not present: BackTranslation, TranslationNote\n",
    "text_types = [\"none\", \"Title\", \"Section\", \"VerseText\", \"NoteText\", \"Other\", \"BackTranslation\", \"TranslationNote\"]\n",
    "\n",
    "# not present: End, MilestoneEnd\n",
    "style_types = [\"none\", \"Character\", \"Note\", \"Paragraph\", \"End\", \"Milestone\", \"MilestoneEnd\"]\n",
    "\n",
    "matrix = [[[] for _ in style_types] for _ in text_types]\n",
    "with open(\"usfm.sty\") as f:\n",
    "    usfm = [l.strip() for l in f.readlines()]\n",
    "\n",
    "marker = \"\"\n",
    "tt = 0\n",
    "st = 0\n",
    "for line in usfm:\n",
    "    if line.startswith(\"\\\\Marker\"):\n",
    "        if len(marker) > 0:\n",
    "            matrix[tt][st].append(marker)\n",
    "        marker = line.split()[1]\n",
    "    if line.startswith(\"\\\\TextType\"):\n",
    "        try:\n",
    "            tt = text_types.index(line.split()[1])\n",
    "        except:\n",
    "            tt = 0\n",
    "    if line.startswith(\"\\\\StyleType\"):\n",
    "        try:\n",
    "            st = style_types.index(line.split()[1])\n",
    "        except:\n",
    "            st = 0\n",
    "\n",
    "print(\"\\t\" + \"\\t\".join(style_types))\n",
    "for i, tt in enumerate(matrix):\n",
    "    print(text_types[i] + \"\\t\" + \"\\t\".join([\",\".join(st) for st in tt]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
