{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment w/ more aligning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silnlp.common.corpus import load_corpus, write_corpus\n",
    "from silnlp.alignment.config import get_aligner\n",
    "from silnlp.alignment.machine_aligner import MachineAligner\n",
    "from typing import List, Union\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from machine.tokenization import LatinWordTokenizer\n",
    "from machine.corpora import escape_spaces, nfc_normalize, lowercase\n",
    "\n",
    "# Tokenize and normalize sentences in the same way as the normal alignment process\n",
    "def tokenize_for_alignment(sents: List[Union[str, List[str]]]) -> List[str]:\n",
    "    if type(sents[0]) == str:\n",
    "        tokenizer = LatinWordTokenizer()\n",
    "        sents = [tokenizer.tokenize(sent) for sent in sents]\n",
    "    sents_norm = [lowercase(nfc_sent) for nfc_sent in [nfc_normalize(es_sent) for es_sent in [escape_spaces(sent) for sent in sents]]]\n",
    "    return [\" \".join(toks) for toks in sents_norm]\n",
    "\n",
    "def align_sents(src_sents: List[str],\n",
    "                trg_sents: List[str],\n",
    "                aligner_id: str = \"hmm\",\n",
    "                sym_align_path: Path = None,\n",
    "                extra_train_data_src: Path = None,\n",
    "                extra_train_data_trg: Path = None\n",
    "                ) -> None:\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        temp_dir = Path(td)\n",
    "\n",
    "        # Since the sentences get tokenized before they are normalized in the normal alignment process,\n",
    "        # we can wait to do the normalization until here\n",
    "        src_sents_norm = tokenize_for_alignment(src_sents)\n",
    "        trg_sents_norm = tokenize_for_alignment(trg_sents)\n",
    "\n",
    "        # Prep alignment data\n",
    "        align_src_path = temp_dir / \"align.src.txt\"\n",
    "        align_trg_path = temp_dir / \"align.trg.txt\"\n",
    "        write_corpus(align_src_path, src_sents_norm)\n",
    "        write_corpus(align_trg_path, trg_sents_norm)\n",
    "\n",
    "        # Prep training data\n",
    "        if extra_train_data_src and extra_train_data_trg:\n",
    "            src_sents_norm += tokenize_for_alignment(list(load_corpus(extra_train_data_src)))\n",
    "            trg_sents_norm += tokenize_for_alignment(list(load_corpus(extra_train_data_trg)))\n",
    "        train_src_path = temp_dir / \"train.src.txt\"\n",
    "        train_trg_path = temp_dir / \"train.trg.txt\"\n",
    "        write_corpus(train_src_path, src_sents_norm)\n",
    "        write_corpus(train_trg_path, trg_sents_norm)\n",
    "\n",
    "        # Train the aligner and align\n",
    "        aligner: MachineAligner = get_aligner(aligner_id, temp_dir)\n",
    "\n",
    "        aligner.train(train_src_path, train_trg_path)\n",
    "        aligner.force_align(align_src_path, align_trg_path, sym_align_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USFM marker preservation\n",
    "* Extract footnotes and put them at the end\n",
    "* Extract each instance of a marker and record its index\n",
    "* Tokenize source sentences and match each marker to surrounding tokens based on their original indices\n",
    "* Train aligner on all training data + translation, align translation to source\n",
    "* Reinsert marker instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "'''Define project values'''\n",
    "pair = \"\"\n",
    "project = \"\"\n",
    "file_suffix = \"\"\n",
    "trg_project = \"\"\n",
    "trg_file_suffix = \"\"\n",
    "\n",
    "book = \"JHN\"\n",
    "book_name = f\"44{book}\"\n",
    "src_fpath = Path(f\"test_S/Paratext/projects/{project}/{book_name}{file_suffix}.SFM\")\n",
    "aligner = \"eflomal\"\n",
    "pair_dir = Path(f\"zzz_PN_KTs/{pair}\")\n",
    "align_path = pair_dir / book / f\"{book_name}_sym-align_{aligner}.txt\"\n",
    "out_fpath = pair_dir / book / f\"{book_name}{trg_file_suffix}_out.SFM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import FileParatextProjectSettingsParser, UsfmFileText, ScriptureRef\n",
    "from machine.tokenization import LatinWordTokenizer\n",
    "\n",
    "src_settings = FileParatextProjectSettingsParser(src_fpath.parent).parse()\n",
    "src_file_text = UsfmFileText(\n",
    "    src_settings.stylesheet,\n",
    "    src_settings.encoding,\n",
    "    book,\n",
    "    src_fpath,\n",
    "    src_settings.versification,\n",
    "    include_markers=True, # F/T gives notes their own rows, F/F gives just the main text, T/F gives one ref per verse and all markers are inline\n",
    "    include_all_text=True, # T/T includes all intro and section titles (as does F/T), all other notes/markers inline\n",
    "    project=src_settings.name,\n",
    ")\n",
    "\n",
    "sentences = []\n",
    "vrefs = []\n",
    "for sent in src_file_text:\n",
    "    if len(sent.ref.path) > 0 and sent.ref.path[-1].name == \"rem\":\n",
    "        continue\n",
    "    sentences.append(sent.text.strip())\n",
    "    vrefs.append(sent.ref)\n",
    "\n",
    "# orig_sents = sentences.copy()\n",
    "# After all the markers are removed, these lists will be a mapping of the indices of the remaining characters to their original indices\n",
    "# orig_indices = [[i for i in range(len(sent))] for sent in sentences]\n",
    "\n",
    "for ref, sent in zip(vrefs, sentences):\n",
    "    print(ref, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Extract inline notes'''\n",
    "# note_markers = [\"f\", \"fm\", \"rq\", \"xtSeeAlso\"] # + all in stylesheet w/ TextType:NoteText\n",
    "# INLINE_NOTE = re.compile(r\"\\\\({}) (.*?)\\\\\\1\\*\".format(\"|\".join(note_markers)))\n",
    "# inline_notes = []\n",
    "# for i, (ref, sent) in enumerate(zip(vrefs, sentences)):\n",
    "#     idxs_to_remove = []\n",
    "#     for match in INLINE_NOTE.finditer(sent):\n",
    "#         marker, content = match.groups()\n",
    "#         # Record index and length of note\n",
    "#         idxs_to_remove += [j for j in range(match.start() + 1, match.start() + len(match[0]))]  # +1 from replacing the match w/ a space\n",
    "#         # Record original index and segment info. At this point, orig_indices[i][match.start()] == match.start()\n",
    "#         inline_notes.append((i, orig_indices[i][match.start()], len(match[0]), ref, marker, content))\n",
    "#         sentences[i] = sentences[i].replace(match[0], \" \", 1)\n",
    "#     for idx in reversed(idxs_to_remove):\n",
    "#         orig_indices[i].pop(idx)\n",
    "# # Add notes to the list of sentences so they're translated separately\n",
    "# for _, _, _, ref, marker, content in inline_notes:\n",
    "#     # as long as the vref is right, can put at the end of the list so it doesn't mess with indices of the \"index recovery\" alg\n",
    "#     sentences.append(content)\n",
    "#     vrefs.append(ref)  # TODO: update ref for new sent\n",
    "#     # seen_notes.append([])\n",
    "#     orig_indices.append([i for i in range(len(content))])\n",
    "\n",
    "\n",
    "# '''Extract all instances of markers'''\n",
    "# INLINE_MARKER = re.compile(r\"\\\\(\\S+?(?:\\*| ))\")\n",
    "# inline_markers = []\n",
    "# for i, (ref, sent) in enumerate(zip(vrefs, sentences)):\n",
    "#     idxs_to_remove = []\n",
    "#     for match in INLINE_MARKER.finditer(sent):\n",
    "#         marker, = match.groups()\n",
    "#         # Record index and length of markers\n",
    "#         idxs_to_remove += [j for j in range(match.start() + 1, match.start() + len(match[0]))] # +1 from replacing the match w/ a space\n",
    "#         # Record original index and segment info\n",
    "#         inline_markers.append((i, orig_indices[i][match.start()], len(match[0]), ref, match[0])) # not all of this is necessary\n",
    "#         sentences[i] = sentences[i].replace(match[0], \" \", 1)\n",
    "#     for idx in reversed(idxs_to_remove):\n",
    "#         orig_indices[i].pop(idx)\n",
    "\n",
    "# print(\"sent_idx, orig_idx, len, ref, marker, content\")\n",
    "# for i in range(min(5,len(inline_notes))):\n",
    "#     print(inline_notes[i])\n",
    "# print(\"sent_idx, orig_idx, len, ref, marker\")\n",
    "# for i in range(min(5,len(inline_markers))):\n",
    "#     print(inline_markers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only deal with paragraph markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import UsfmTokenizer, UsfmTokenType\n",
    "\n",
    "'''Parse sentences'''\n",
    "tokenizer = UsfmTokenizer(src_settings.stylesheet)\n",
    "sentence_toks = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "to_delete = [\"fig\"]\n",
    "inline_markers = []\n",
    "# markers_by_verse = [[] for _ in sentence_toks]\n",
    "text_only_sents = [\"\" for _ in sentence_toks]\n",
    "sentence_text_toks = [[] for _ in sentence_toks]\n",
    "for i, toks in enumerate(sentence_toks):\n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(toks): # POSSIBLE TYPES: TEXT, PARAGRAPH, CHARACTER, NOTE, END\n",
    "        if ignore_scope is not None:\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignore_scope = tok\n",
    "        elif tok.type == UsfmTokenType.PARAGRAPH:\n",
    "            # len of text so far == start idx of next text tok, this won't break with other markers bc the idx is wrt only the text toks \n",
    "            inline_markers.append((i, len(text_only_sents[i]), tok.to_usfm())) # previously tok.to_usfm() so it could be inserted into the text\n",
    "            # markers_by_verse[i].append(tok.to_usfm())\n",
    "        elif tok.type == UsfmTokenType.TEXT:\n",
    "            text_only_sents[i] += tok.text\n",
    "            sentence_text_toks[i].append(tok)\n",
    "        elif tok.type != UsfmTokenType.CHARACTER and tok.type != UsfmTokenType.END:\n",
    "            print(tok.type, tok)\n",
    "\n",
    "print(\"sent_idx, orig_idx, marker\")\n",
    "for marker in inline_markers:\n",
    "    print(marker)\n",
    "# for ref, markers in zip(vrefs, markers_by_verse):\n",
    "#     if len(markers) > 0:\n",
    "#         print(ref, markers)\n",
    "# for ref, sent in zip(vrefs, text_only_sents):\n",
    "#     print(ref, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import FileParatextProjectSettingsParser, UsfmFileText\n",
    "from pathlib import Path\n",
    "\n",
    "'''Translate sentences (use target sentences for now)'''\n",
    "\n",
    "# Get target file and remove all markers\n",
    "trg_file_path = Path(f\"test_S/Paratext/projects/{trg_project}/{book_name}{trg_file_suffix}.SFM\")\n",
    "trg_settings = FileParatextProjectSettingsParser(trg_file_path.parent).parse()\n",
    "trg_file_text = UsfmFileText(\n",
    "    trg_settings.stylesheet,\n",
    "    trg_settings.encoding,\n",
    "    trg_settings.get_book_id(trg_file_path.name),\n",
    "    trg_file_path,\n",
    "    trg_settings.versification,\n",
    "    include_markers=True,\n",
    "    include_all_text=True,\n",
    "    project=trg_settings.name,\n",
    ")\n",
    "tokenizer = UsfmTokenizer(trg_settings.stylesheet)\n",
    "trg_sents = []\n",
    "for sent in trg_file_text:\n",
    "    if len(sent.ref.path) > 0 and sent.ref.path[-1].name == \"rem\":\n",
    "        continue\n",
    "    trg_sents.append(\"\")\n",
    "\n",
    "    sent = sent.text.strip()\n",
    "    usfm_toks = tokenizer.tokenize(sent)\n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(usfm_toks): # POSSIBLE TYPES: TEXT, PARAGRAPH, CHARACTER, NOTE, END\n",
    "        if ignore_scope is not None:\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignore_scope = tok\n",
    "        elif tok.type == UsfmTokenType.TEXT:\n",
    "            trg_sents[-1] += tok.text\n",
    "\n",
    "for ref, sent in zip(vrefs,trg_sents):\n",
    "    print(ref, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "'''Tokenize sentences'''\n",
    "tokenizer = LatinWordTokenizer()\n",
    "\n",
    "# TODO: force ranges to not cross boundaries by tokenizing each text token separately? or bound the alignment range some other way based on surrounding markers\n",
    "# is there a simpler way to force markers to be reinserted in the same relative order?\n",
    "word_tok_ranges = [list(tokenizer.tokenize_as_ranges(sent)) for sent in text_only_sents]\n",
    "toks = [[sent[r.start : r.end] for r in ranges] for sent, ranges in zip(text_only_sents, word_tok_ranges)]\n",
    "\n",
    "trg_word_tok_ranges = [list(tokenizer.tokenize_as_ranges(sent)) for sent in trg_sents]\n",
    "trg_toks = [[sent[r.start : r.end] for r in ranges] for sent, ranges in zip(trg_sents, trg_word_tok_ranges)]\n",
    "\n",
    "'''Match markers to the token closest to them'''\n",
    "# TODO: need to disambiguate the order of markers in the case where there are multiple markers in a row\n",
    "# Returns a list of the indices of the the tokens following each of the input sequences\n",
    "# The returned indices are based on the tokens post-marker removal, but the lookup is based on the character indices of the original strings\n",
    "def get_toks_after_sequences(sequences: List[Tuple]) -> List[int]:\n",
    "    toks_after_seqs = []\n",
    "    for sequence in sequences:\n",
    "        sent_idx, start_idx = sequence[0], sequence[1] # now, start_idx is wrt text_only_sents\n",
    "        for i, tok_range in reversed(list(enumerate(word_tok_ranges[sent_idx]))):\n",
    "            # this works fine but there's still risk of splitting words if the token goes across paragraph boundaries\n",
    "            # this can be fixed by forcing tok boundaries to stay within usfm text toks\n",
    "            if tok_range.start <= start_idx or i == 0:\n",
    "                toks_after_seqs.append(i)\n",
    "                break\n",
    "\n",
    "    return toks_after_seqs\n",
    "toks_after_markers = get_toks_after_sequences(inline_markers)\n",
    "\n",
    "'''Test alg'''\n",
    "'''out_sents = text_only_sents.copy()\n",
    "for i in reversed(range(len(inline_markers))):\n",
    "    sent_idx = inline_markers[i][0]\n",
    "    insert_idx = word_tok_ranges[sent_idx][toks_after_markers[i]].start\n",
    "    out_sents[sent_idx] = out_sents[sent_idx][:insert_idx] + inline_markers[i][2] + out_sents[sent_idx][insert_idx:]\n",
    "rows = [([ref], sent) for ref, sent in zip(vrefs, out_sents)]\n",
    "dest_updater = FileParatextProjectTextUpdater(src_fpath.parent)\n",
    "usfm_out = dest_updater.update_usfm(\n",
    "    src_file_text.id, rows, strip_all_text=True, prefer_existing_text=False\n",
    ")\n",
    "with open(f\"zzz_PN_KTs/{pair}/{book}/test_para_idxs.SFM\", \"w\", encoding=src_settings.encoding) as f:\n",
    "    f.write(usfm_out)'''\n",
    "\n",
    "print(word_tok_ranges[0])\n",
    "print(toks[0])\n",
    "print(trg_word_tok_ranges[0])\n",
    "print(trg_toks[0])\n",
    "print(toks_after_markers)\n",
    "# for i, (sent_idx, _, marker) in enumerate(inline_markers):\n",
    "#     out = toks[i][max(0,toks_after_markers[i]-3):min(len(toks[i]),toks_after_markers[i])]\n",
    "#     if len(out) == 0:\n",
    "#         print(inline_markers[i])\n",
    "#     else:\n",
    "#         print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silnlp.alignment.utils import compute_alignment_scores\n",
    "from silnlp.common.corpus import write_corpus\n",
    "'''Align sentences'''\n",
    "# align_sents(toks, trg_toks, aligner, align_path) # Path(f\"zzz_PN_KTs/tpi_aps/train.src.detok.txt\"),Path(f\"zzz_PN_KTs/tpi_aps/train.trg.detok.txt\")\n",
    "\n",
    "# eflomal\n",
    "if not align_path.exists():\n",
    "    src_corpus = write_corpus(pair_dir / book / \"src_align.txt\", text_only_sents) # orig sentences\n",
    "    trg_corpus = write_corpus(pair_dir / book / \"trg_align.txt\", trg_sents)\n",
    "    compute_alignment_scores(pair_dir / book / \"src_align.txt\", pair_dir / book / \"trg_align.txt\", aligner, align_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from silnlp.common.corpus import load_corpus\n",
    "\n",
    "# probably non-exhaustive, second of same-looking single closing quotes is an apostrophe, I think\n",
    "# QUOTATION_MARKS = [\"'\", '\"', \"“\", \"”\", \"‘\", \"’\", \"ʼ\", \"<\", \">\"]\n",
    "\n",
    "'''Decide where to reinsert markers'''\n",
    "if aligner == \"eflomal\":\n",
    "    align_lines = [[(lambda x: (int(x[0]), int(x[1])))(pair.split(\"-\")) for pair in line.split()] for line in load_corpus(align_path)]\n",
    "else:\n",
    "    align_lines = [[(lambda x: (int(x[0]), int(x[1])))(pair.split(\":\")[0].split(\"-\")) for pair in line.split()] for line in load_corpus(align_path)]\n",
    "\n",
    "# Gets the number of alignment pairs that \"cross the line\" between (src_idx - .5) and (trg_idx - .5)\n",
    "def num_align_crossings(sent_idx: int, src_idx: int, trg_idx: int) -> int:\n",
    "    crossings = 0\n",
    "    for i,j in align_lines[sent_idx]:\n",
    "        if i < src_idx and j >= trg_idx:\n",
    "            crossings += 1\n",
    "        if i >= src_idx and j < trg_idx:\n",
    "            crossings += 1\n",
    "    return crossings\n",
    "\n",
    "# Get the index of the trg token that each sequence should be inserted before\n",
    "def get_insert_indices(seqs_to_insert: List[Tuple], adj_tok_idxs: List[int]) -> List[int]:\n",
    "    punct_hyp_freqs = defaultdict(int)\n",
    "    hyp_freqs = defaultdict(int)\n",
    "    \n",
    "    insert_indices = []\n",
    "    for seq, adj_src_tok in zip(seqs_to_insert, adj_tok_idxs):\n",
    "        sent_idx = seq[0]\n",
    "\n",
    "        '''\n",
    "        Alternative approach\n",
    "        for each 'threshold' 0 to 5, for each start position offset in [0,1,2,-1,-2], for each trg token the src is aligned, \n",
    "        return the first pairing that matches the threshold\n",
    "        '''\n",
    "        # # hyps = [0, 1, 2, -1, -2]\n",
    "        # hyps = [0, 1, -1, 2, -2]\n",
    "        # max_crossings = 0\n",
    "        # insert_idx = -1\n",
    "        # while insert_idx == -1 and max_crossings < 5: # if there's a limit, just make a for loop\n",
    "        #     for hyp in hyps:\n",
    "        #         src_hyp = adj_src_tok + hyp\n",
    "\n",
    "        #         # this will also filter out OOB hypotheses\n",
    "        #         trg_hyps = [t for (s,t) in align_lines[sent_idx] if s == src_hyp]\n",
    "        #         # print(len(trg_hyps))\n",
    "        #         if len(trg_hyps) == 0:\n",
    "        #             continue\n",
    "\n",
    "        #         if hyp < 0:\n",
    "        #             trg_hyps = reversed(trg_hyps)\n",
    "                \n",
    "        #         for i, trg_hyp in enumerate(trg_hyps, 1):\n",
    "        #             if num_align_crossings(sent_idx, src_hyp, trg_hyp) <= max_crossings:\n",
    "        #                 insert_idx = (hyp, i, trg_hyp) # hyp and i are just for tracking purposes, can just have trg_hyp later\n",
    "        #                 break\n",
    "\n",
    "        #         if insert_idx != -1:\n",
    "        #             break\n",
    "        #     max_crossings += 1\n",
    "\n",
    "        # insert_indices.append(insert_idx) # could be -1, need to handle outside of function\n",
    "\n",
    "        '''Revision of original, checks first hypothesis of its original position plus one on either side and picks the best'''\n",
    "        '''# # only 0,1,2 was close, checking for prev tokens seems to always be bad\n",
    "        # hyps = [0, 1, 2]\n",
    "        # best_hyp = (-1, None, len(align_lines[sent_idx]))\n",
    "        # for hyp in hyps:\n",
    "        #     src_hyp = adj_src_tok + hyp\n",
    "        #     trg_hyps = [t for (s,t) in align_lines[sent_idx] if s == src_hyp]\n",
    "        #     if len(trg_hyps) == 0:\n",
    "        #         continue\n",
    "\n",
    "        #     trg_hyp = trg_hyps[-1] if hyp < 0 else trg_hyps[0]\n",
    "        #     num_crossings = num_align_crossings(sent_idx, src_hyp, trg_hyp)\n",
    "        #     if num_crossings < best_hyp[2]:\n",
    "        #         best_hyp = (trg_hyp, hyp, num_crossings)\n",
    "        # if best_hyp[0] == -1:\n",
    "        #     insert_indices.append(len(trg_toks[sent_idx])) # insert at the end of the sentence\n",
    "        #     hyp_freqs[\"none\"] += 1\n",
    "        # else:\n",
    "        #     insert_indices.append(best_hyp[0])\n",
    "        #     hyp_freqs[best_hyp[1]] += 1'''\n",
    "\n",
    "        '''Original, checks first hypothesis of its original position and one tok previous and picks the better of the two'''\n",
    "        # If the token on either side of a hypothesis is punctuation, use that\n",
    "        # can also try a less extreme version where the punct hyps are still subject to having the least crossings, but they are still always checked first\n",
    "\n",
    "        trg_hyp = -1\n",
    "        punch_hyps = [-1, 0]\n",
    "        for punct_hyp in punch_hyps:\n",
    "            src_hyp = adj_src_tok + punct_hyp\n",
    "            if src_hyp < 0 or src_hyp >= len(toks[sent_idx]):\n",
    "                continue\n",
    "            # only accept pairs where both the src and trg token are punct\n",
    "            # can define more specifically what the punct tokens can look like later\n",
    "            if len(toks[sent_idx][src_hyp]) > 0 and not any(c.isalpha() for c in toks[sent_idx][src_hyp]):\n",
    "                align_pairs = reversed(align_lines[sent_idx]) if punct_hyp < 0 else align_lines[sent_idx]\n",
    "                for s,t in align_pairs:\n",
    "                    if s == src_hyp and not any(c.isalpha() for c in trg_toks[sent_idx][t]):\n",
    "                        trg_hyp = t\n",
    "                        break\n",
    "            if trg_hyp != -1:\n",
    "                # if this search gets expanded beyond [-1,0] can do insert_idx -= punct_hyp\n",
    "                insert_idx = trg_hyp + 1 if punct_hyp < 0 else trg_hyp\n",
    "                insert_indices.append(insert_idx)\n",
    "                punct_hyp_freqs[punct_hyp] += 1\n",
    "                break\n",
    "        if trg_hyp != -1:\n",
    "            continue\n",
    "\n",
    "        # hyps = [0, -1] # offsets to test, original and previous\n",
    "        hyps = [0, 1, 2]\n",
    "        # hyps = [0, 1, -1, 2, -2]\n",
    "        # hyps = [0, -1, 1, -2, 2]\n",
    "        # hyps = [-1, 0, 1, -2, 2]\n",
    "        # hyps = [0, 1, 2, -1, -2]\n",
    "        best_hyp = (-1, None, len(align_lines[sent_idx]))\n",
    "        checked = set() # to prevent checking the same idx twice\n",
    "        for hyp in hyps:\n",
    "            align_pairs = reversed(align_lines[sent_idx]) if hyp < 0 else align_lines[sent_idx]\n",
    "            src_hyp = adj_src_tok + hyp\n",
    "            if src_hyp in checked:\n",
    "                continue\n",
    "            trg_hyp = -1\n",
    "            while trg_hyp == -1 and src_hyp >= 0 and src_hyp < len(toks[sent_idx]):\n",
    "                checked.add(src_hyp)\n",
    "                trg_hyps = [t for (s,t) in align_pairs if s == src_hyp]\n",
    "                if len(trg_hyps) > 0:\n",
    "                    trg_hyp = trg_hyps[0]\n",
    "                else:\n",
    "                    src_hyp += -1 if hyp < 0 else 1\n",
    "            if trg_hyp != -1:\n",
    "                num_crossings = num_align_crossings(sent_idx, src_hyp, trg_hyp)\n",
    "                if num_crossings < best_hyp[2]:\n",
    "                    # replace hyp with src_hyp - adj_src_tok\n",
    "                    # this is what I was trying to do before with offsetting the farther away hyps, but this way is worse\n",
    "                    best_hyp = (trg_hyp, hyp, num_crossings)\n",
    "\n",
    "        if best_hyp[0] == -1:\n",
    "            insert_indices.append(len(trg_toks[sent_idx])) # insert at the end of the sentence\n",
    "            hyp_freqs[\"none\"] += 1\n",
    "        else:\n",
    "            insert_idx = best_hyp[0]\n",
    "            # do this before or after subtracting the offset? only subtract offset if not adjacent to punct? check before and after subtraction?\n",
    "            if trg_toks[sent_idx][insert_idx] in [\",\", \".\", \"!\", \"?\"]:\n",
    "                insert_idx += 1\n",
    "            # subtracting hyp at the end may be bad in other cases, or make wrong answers worse/more confusing\n",
    "            else:\n",
    "                insert_idx = insert_idx - best_hyp[1] #  - best_hyp[1]\n",
    "                if trg_toks[sent_idx][insert_idx] in [\",\", \".\", \"!\", \"?\"]:\n",
    "                    insert_idx += 1\n",
    "            \n",
    "            insert_indices.append(insert_idx)\n",
    "            hyp_freqs[best_hyp[1]] += 1\n",
    "\n",
    "    print(punct_hyp_freqs)\n",
    "    print(hyp_freqs)\n",
    "\n",
    "    # ret = []\n",
    "    # for i, result in enumerate(insert_indices):\n",
    "    #     if result == -1:\n",
    "    #         print(i, vrefs[seqs_to_insert[i][0]], seqs_to_insert[i])\n",
    "    #         # ret.append(-1)\n",
    "    #         ret.append(len(trg_toks[seqs_to_insert[i][0]]))\n",
    "    #     else:\n",
    "    #         hyp, hyp_num, idx = result\n",
    "    #         print(hyp, hyp_num) # how many tokens away from marker, how far away the first alignment w/ 0 crossings was\n",
    "    #         ret.append(idx)\n",
    "    \n",
    "    return insert_indices\n",
    "\n",
    "trg_toks_after_markers = get_insert_indices(inline_markers, toks_after_markers)\n",
    "\n",
    "print(toks_after_markers)\n",
    "print(trg_toks_after_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: right now, half the word for the insertion order is done when to_insert is filled out, and the other half is done when the markers are\n",
    "being inserted (with reverse). Since there there's already funky stuff going on in the order of the markers in to_insert (for disambiguation \n",
    "for the same insertion idx), it would make more sense to just do all the ordering when to_insert is being filled out, i.e. the order of a list\n",
    "in to_insert is the order they need to be inserted in\n",
    "'''\n",
    "\n",
    "'''Reinsert markers'''\n",
    "to_insert = [[] for _ in vrefs]\n",
    "\n",
    "# Collect the markers to be inserted\n",
    "for i, (mark, next_trg_tok) in enumerate(zip(inline_markers, trg_toks_after_markers)):\n",
    "    sent_idx, _, marker = mark\n",
    "    if next_trg_tok >= len(trg_word_tok_ranges[sent_idx]): # TODO: this shouldn't happen\n",
    "        insert_idx = len(trg_sents[sent_idx])\n",
    "    else:\n",
    "        insert_idx = trg_word_tok_ranges[sent_idx][next_trg_tok].start\n",
    "    \n",
    "    # figure out the order of the markers in the sentence to handle ambiguity for directly adjacent markers\n",
    "    insert_place = 0\n",
    "    while insert_place < len(to_insert[sent_idx]) and to_insert[sent_idx][insert_place][0] <= insert_idx:\n",
    "        insert_place += 1\n",
    "\n",
    "    to_insert[sent_idx].insert(insert_place, (insert_idx, marker))  # \"\\n\" + marker\n",
    "\n",
    "'''construct to_insert with the actual strings instead of indices so they can be inserted manually'''\n",
    "# to do this, it would be nice to have the markers for a given sentence appear in the list in the order they appear\n",
    "# for i, (ref, trg_sent, markers) in enumerate(zip(vrefs, trg_sents, to_insert)):\n",
    "#     for marker in reversed(markers):\n",
    "\n",
    "'''original'''\n",
    "# Insert the strings into the target sentences\n",
    "# for sent_idx in range(len(trg_sents)):\n",
    "#     for insert_idx, insert_str in reversed(to_insert[sent_idx]): # sorted(to_insert[sent_idx], key=lambda x: x[0], reverse=True)\n",
    "#         trg_sents[sent_idx] = trg_sents[sent_idx][:insert_idx] + insert_str + trg_sents[sent_idx][insert_idx:]\n",
    "\n",
    "'''create rows for each paragraph marker with specific ScriptureRef paths'''\n",
    "# Construct rows to update the USFM file with\n",
    "rows = []\n",
    "for sent_idx, (ref, trg_sent) in enumerate(zip(vrefs, trg_sents)):\n",
    "    marker_rows = []\n",
    "    for marker_idx, (insert_idx, marker) in reversed(list(enumerate(to_insert[sent_idx], 1))):\n",
    "        # marker_rows.insert(0, ([ScriptureRef(ref.verse_ref, ref.path + [ScriptureElement(marker_idx, marker[1:-1])])], trg_sent[insert_idx:]))\n",
    "        marker_rows.insert(0, ([ref], trg_sent[insert_idx:]))\n",
    "        trg_sent = trg_sent[:insert_idx]\n",
    "    rows.append(([ref], trg_sent))\n",
    "    for m_row in marker_rows:\n",
    "        rows.append(m_row)\n",
    "\n",
    "for ref, sent in rows:\n",
    "    print(ref, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import UpdateUsfmParserHandler, parse_usfm, UsfmParserState, UpdateUsfmBehavior\n",
    "\n",
    "'''class ParagraphUpdateUsfmParserHandler(UpdateUsfmParserHandler):\n",
    "    def _collect_tokens(self, state: UsfmParserState) -> None:\n",
    "        self._tokens.extend(self._new_tokens)\n",
    "        self._new_tokens.clear()\n",
    "        while self._token_index <= state.index + state.special_token_count:\n",
    "            if state.tokens[self._token_index].type == UsfmTokenType.PARAGRAPH:\n",
    "                num_text = 0\n",
    "                for i in range(len(self._tokens) - 1, -1, -1):\n",
    "                    if self._tokens[i].type == UsfmTokenType.TEXT:\n",
    "                        num_text += 1\n",
    "                    else:\n",
    "                        break\n",
    "                if num_text >= 2:\n",
    "                    self._tokens.insert(-(num_text - 1), state.tokens[self._token_index])\n",
    "                    self._token_index += 1\n",
    "                    break # should this be continue instead? what situations are there where \n",
    "            self._tokens.append(state.tokens[self._token_index])\n",
    "            self._token_index += 1'''\n",
    "\n",
    "class ParagraphUpdateUsfmParserHandler(UpdateUsfmParserHandler):\n",
    "    def _collect_tokens(self, state: UsfmParserState) -> None:\n",
    "        self._tokens.extend(self._new_tokens)\n",
    "        self._new_tokens.clear()\n",
    "        while self._token_index <= state.index + state.special_token_count:\n",
    "            if state.tokens[self._token_index].type == UsfmTokenType.PARAGRAPH and state.tokens[self._token_index].marker != \"rem\":\n",
    "                num_text = 0\n",
    "                rem_offset = 0\n",
    "                for i in range(len(self._tokens) - 1, -1, -1):\n",
    "                    if self._tokens[i].type == UsfmTokenType.TEXT:\n",
    "                        num_text += 1\n",
    "                    elif self._tokens[i].type == UsfmTokenType.PARAGRAPH and self._tokens[i].marker == \"rem\":\n",
    "                        rem_offset += num_text + 1\n",
    "                        num_text = 0\n",
    "                    else:\n",
    "                        break\n",
    "                if num_text >= 2:\n",
    "                    self._tokens.insert(-(rem_offset + num_text - 1), state.tokens[self._token_index])\n",
    "                    self._token_index += 1\n",
    "                    break # should this be continue instead? what situations are there where \n",
    "            self._tokens.append(state.tokens[self._token_index])\n",
    "            self._token_index += 1\n",
    "\n",
    "'''Update USFM and write out'''\n",
    "# preserve_whitespace=True doesn't change anything with markers on newlines but it does take care of the \\vp\\*vp somehow\n",
    "with open(src_fpath, encoding=\"utf-8-sig\") as f:\n",
    "    usfm = f.read()\n",
    "handler = ParagraphUpdateUsfmParserHandler(rows, behavior=UpdateUsfmBehavior.PREFER_NEW)\n",
    "parse_usfm(usfm, handler, src_settings.stylesheet, src_settings.versification, preserve_whitespace=False)\n",
    "usfm_out = handler.get_usfm(src_settings.stylesheet)\n",
    "\n",
    "with out_fpath.open(\"w\", encoding=src_settings.encoding) as f:\n",
    "    f.write(usfm_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
