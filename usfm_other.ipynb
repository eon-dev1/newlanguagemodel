{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KT sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import silnlp.common.paratext\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from silnlp.common.corpus import load_corpus\n",
    "\n",
    "# paths for generic Major KT lists\n",
    "metadata_path = Path(\"silnlp/assets/Major-metadata.txt\")\n",
    "vrefs_path = Path(\"silnlp/assets/Major-vrefs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_gloss_path = Path(\"silnlp/assets/fr-Major-glosses.txt\") # en  fr\n",
    "trg_gloss_path = Path(\"test_S/MT/terms/bcw-bcw_2024_02_21-Major-renderings.txt\") # lmp-lmp_2024_02_16  bcw-bcw_2024_02_21\n",
    "pair = \"fr_bcw\" # en_lmp  fr_bcw\n",
    "\n",
    "proper_nouns = defaultdict(dict)\n",
    "for i, (meta, vref, src_gloss, trg_gloss) in enumerate(zip(load_corpus(metadata_path), load_corpus(vrefs_path), load_corpus(src_gloss_path), load_corpus(trg_gloss_path))):\n",
    "    term, pt_cat, sem_cat = meta.split(\"\\t\") # orig lang term, Paratext category (PN, FL, RE, etc.), semantic category (person, grasses, containers, etc.)\n",
    "    instances = vref.split(\"\\t\") # all occurrences of the term\n",
    "    src_glosses = src_gloss.split(\"\\t\") # all potential glosses for term\n",
    "    trg_glosses = trg_gloss.split(\"\\t\")\n",
    "\n",
    "    if pt_cat == \"PN\" and trg_glosses != [\"\"]:\n",
    "        proper_nouns[i][\"glosses\"] = (src_glosses, trg_glosses)\n",
    "        proper_nouns[i][\"instances\"] = instances # might want to give this further structure, i.e. be a dict w/ book:chapter:[instances]\n",
    "\n",
    "with open(f\"zzz_PN_KTs/{pair}/KT_to_vrefs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(proper_nouns, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Create verse-to-KTs dict\n",
    "vref_to_KTs = defaultdict(list)\n",
    "for i, pn_dict in proper_nouns.items():\n",
    "    for vref in pn_dict[\"instances\"]:\n",
    "        vref_to_KTs[vref].append(i)\n",
    "with open(f\"zzz_PN_KTs/{pair}/vref_to_KTs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vref_to_KTs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix KTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silnlp.common.corpus import load_corpus\n",
    "from pathlib import Path\n",
    "from machine.corpora import ScriptureRef\n",
    "from silnlp.alignment.utils import compute_alignment_scores\n",
    "\n",
    "pair = \"\"\n",
    "book_name = \"08RUT\"\n",
    "vrefs = [ScriptureRef.parse(ref) for ref in load_corpus(Path(f\"zzz_PN_KTs/{pair}/{book_name}_vrefs.txt\"))]\n",
    "src_path = Path(f\"zzz_PN_KTs/{pair}/{book_name}_src_sents.txt\")\n",
    "trg_path = Path(f\"zzz_PN_KTs/{pair}/{book_name}_trg_sents.txt\")\n",
    "\n",
    "# always uses LatinWordTokenizer\n",
    "sym_align_path = Path(f\"zzz_PN_KTs/{pair}/{book_name}_sym-align.txt\")\n",
    "scores = compute_alignment_scores(src_path, trg_path, aligner_id=\"eflomal\", sym_align_path=sym_align_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.tokenization import LatinWordTokenizer\n",
    "from machine.corpora import TextFileTextCorpus\n",
    "from machine.scripture import VerseRef\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "# treat_apostrophe_as_single_quote=True didn't do anything\n",
    "# confirmed that these have the same tokenization as the aligner (for Latin script)\n",
    "# aligner uses LatinWordTokenizer + escape_spaces, nfc_normalize, lowercase from TextCorpus\n",
    "src_lines = [line.segment for line in TextFileTextCorpus(src_path).tokenize(LatinWordTokenizer()).lowercase()]\n",
    "trg_lines = [line.segment for line in TextFileTextCorpus(trg_path).tokenize(LatinWordTokenizer())]\n",
    "src_lines_raw = load_corpus(src_path)\n",
    "trg_lines_raw = load_corpus(trg_path)\n",
    "\n",
    "align_lines = [[(lambda x: (int(x[0]), int(x[1])))(pair.split(\":\")[0].split(\"-\")) for pair in line.split()] for line in load_corpus(sym_align_path)]\n",
    "\n",
    "# # check for alignment coverage\n",
    "# # not complete coverage, so can't assume anything about if specific words are aligned\n",
    "# for i, (ref,src_line,trg_line,align_pairs) in enumerate(zip(vrefs, src_lines, trg_lines, align_lines)):\n",
    "#     src_idxs = {pair[0] for pair in align_pairs}\n",
    "#     trg_idxs = {pair[1] for pair in align_pairs}\n",
    "\n",
    "#     print(i+1, ref)\n",
    "#     print(f\"unaligned SRC: {len(src_line) - len(src_idxs)}\")\n",
    "#     print(f\"unaligned TRG: {len(trg_line) - len(trg_idxs)}\")\n",
    "\n",
    "pair = \"en_lmp\"\n",
    "book = \"RUT\"\n",
    "with open(f\"zzz_PN_KTs/{pair}/vref_to_KTs.json\", encoding=\"utf-8\") as f:\n",
    "    vref_to_KTs = json.load(f)\n",
    "with open(f\"zzz_PN_KTs/{pair}/KT_to_vrefs.json\", encoding=\"utf-8\") as f:\n",
    "    KT_to_vrefs = json.load(f)\n",
    "\n",
    "term_ids = set()\n",
    "exp_vrefs = set()\n",
    "for ref, ids in vref_to_KTs.items():\n",
    "    if VerseRef.from_string(ref).book == book:\n",
    "        term_ids.update(ids)\n",
    "        exp_vrefs.add(ref)\n",
    "src_terms = set()\n",
    "trg_terms = set()\n",
    "for id in term_ids:\n",
    "    src_terms.update(KT_to_vrefs[str(id)][\"glosses\"][0])\n",
    "    trg_terms.update(KT_to_vrefs[str(id)][\"glosses\"][1])\n",
    "print(src_terms)\n",
    "\n",
    "# found = defaultdict(list)\n",
    "for ref,src_line,trg_line,align_pairs,trg_line_raw in zip(vrefs, src_lines, trg_lines, align_lines,trg_lines_raw):\n",
    "    if str(ref.verse_ref) not in vref_to_KTs.keys():\n",
    "        continue\n",
    "    if ref.verse_num == 0 or ref.path[0].name != \"\": # the ScriptureRefs I'm testing with have an empty ScriptureElement in the path so is_verse doesn't work\n",
    "        continue\n",
    "\n",
    "    found = []\n",
    "    for term_id in vref_to_KTs[str(ref.verse_ref)]:\n",
    "        glosses = [gloss.lower() for gloss in KT_to_vrefs[str(term_id)][\"glosses\"][0]]\n",
    "        min_dist = (0, 0, 100) # gloss idx of closest match, tok idx of closest match, distance\n",
    "        for i, gloss in enumerate(glosses): # could adjust this to look at n-grams, where n is the number of words in the gloss\n",
    "            for j, tok in enumerate(src_line):\n",
    "                if (j, term_id) in found:\n",
    "                    continue\n",
    "                dist = nltk.edit_distance(gloss, tok) / len(tok)\n",
    "                if dist < min_dist[2]:\n",
    "                    min_dist = (i, j, dist)\n",
    "        # print(glosses[min_dist[0]], src_line[min_dist[1]], min_dist[2])\n",
    "        if min_dist[2] < .3:\n",
    "            found.append((min_dist[1], term_id))\n",
    "    # print(ref)\n",
    "    # print(len(found), len(vref_to_KTs[str(ref.verse_ref)]))\n",
    "    # print(found)\n",
    "    # print(vref_to_KTs[str(ref.verse_ref)])\n",
    "    # print(src_line)\n",
    "\n",
    "    # replace word(s) in target text\n",
    "    for src_idx, term_id in found:\n",
    "        trg_idxs = [pair[1] for pair in align_pairs if pair[0] == src_idx]\n",
    "        print(src_idx, trg_idxs)\n",
    "        print(src_line[src_idx], [trg_line[idx] for idx in trg_idxs])\n",
    "        print(trg_line_raw)\n",
    "        print(trg_line)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Test Files\n",
    "* Remove character markers and delete notes entriely\n",
    "* Leave paragraph markers alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import (\n",
    "    FileParatextProjectSettingsParser, \n",
    "    UsfmFileText, \n",
    "    UpdateUsfmParserHandler, \n",
    "    UsfmTokenizer, \n",
    "    UsfmTokenType, \n",
    "    parse_usfm, \n",
    "    UsfmParserState,\n",
    "    UpdateUsfmBehavior\n",
    "    )\n",
    "from pathlib import Path\n",
    "\n",
    "class ParagraphUpdateUsfmParserHandler(UpdateUsfmParserHandler):\n",
    "    def _collect_tokens(self, state: UsfmParserState) -> None:\n",
    "        self._tokens.extend(self._new_tokens)\n",
    "        self._new_tokens.clear()\n",
    "        while self._token_index <= state.index + state.special_token_count:\n",
    "            if state.tokens[self._token_index].type == UsfmTokenType.PARAGRAPH and state.tokens[self._token_index].marker != \"rem\":\n",
    "                num_text = 0\n",
    "                rem_offset = 0\n",
    "                for i in range(len(self._tokens) - 1, -1, -1):\n",
    "                    if self._tokens[i].type == UsfmTokenType.TEXT:\n",
    "                        num_text += 1\n",
    "                    elif self._tokens[i].type == UsfmTokenType.PARAGRAPH and self._tokens[i].marker == \"rem\":\n",
    "                        rem_offset += num_text + 1\n",
    "                        num_text = 0\n",
    "                    else:\n",
    "                        break\n",
    "                if num_text >= 2:\n",
    "                    self._tokens.insert(-(rem_offset + num_text - 1), state.tokens[self._token_index])\n",
    "                    self._token_index += 1\n",
    "                    break # should this be continue instead? what situations are there where \n",
    "            self._tokens.append(state.tokens[self._token_index])\n",
    "            self._token_index += 1\n",
    "\n",
    "pair = \"\"\n",
    "project = \"\"\n",
    "file_suffix = \"\"\n",
    "\n",
    "char_markers = True\n",
    "\n",
    "book = \"JHN\"\n",
    "book_name = f\"44{book}\"\n",
    "src_file_path = Path(f\"test_S/Paratext/projects/{project}/{book_name}{file_suffix}.SFM\")\n",
    "out_file_path = Path(f\"zzz_PN_KTs/{pair}/{book}/{book_name}{file_suffix}_goal{'_char' if char_markers else ''}.SFM\")\n",
    "src_settings = FileParatextProjectSettingsParser(src_file_path.parent).parse()\n",
    "src_file_text = UsfmFileText(\n",
    "    src_settings.stylesheet,\n",
    "    src_settings.encoding,\n",
    "    src_settings.get_book_id(src_file_path.name),\n",
    "    src_file_path,\n",
    "    src_settings.versification,\n",
    "    include_markers=True,\n",
    "    include_all_text=True,\n",
    "    project=src_settings.name,\n",
    ")\n",
    "\n",
    "tokenizer = UsfmTokenizer(src_settings.stylesheet)\n",
    "sentence_toks = [tokenizer.tokenize(sent.text.strip()) for sent in src_file_text]\n",
    "vrefs = [s.ref for s in src_file_text]\n",
    "\n",
    "to_delete = [\"fig\"]\n",
    "out_toks = [[[]] for _ in sentence_toks]\n",
    "for i, toks in enumerate(sentence_toks):\n",
    "    ignore_scope = None\n",
    "    for j, tok in enumerate(toks): # POSSIBLE TYPES: TEXT, PARAGRAPH, CHARACTER, NOTE, END, ATTRIBUTE\n",
    "        if ignore_scope is not None:\n",
    "            if tok.type == UsfmTokenType.END and tok.marker[:-1] == ignore_scope.marker:\n",
    "                ignore_scope = None\n",
    "        elif tok.type == UsfmTokenType.NOTE or (tok.type == UsfmTokenType.CHARACTER and tok.marker in to_delete):\n",
    "            ignore_scope = tok\n",
    "        elif tok.type == UsfmTokenType.PARAGRAPH:\n",
    "            out_toks[i].append([])\n",
    "        elif (tok.type == UsfmTokenType.TEXT\n",
    "              or (char_markers and (tok.type == UsfmTokenType.CHARACTER or tok.type == UsfmTokenType.END))):\n",
    "            out_toks[i][-1].append(tok)\n",
    "\n",
    "rows = []\n",
    "for ref, sent in zip(vrefs, out_toks):\n",
    "    for segment in sent:\n",
    "        rows.append(([ref], \"\".join([tok.to_usfm() for tok in segment])))\n",
    "\n",
    "'''Update file and write out'''\n",
    "# rows = [([ref], \"\".join([tok.to_usfm() for tok in sent])) for ref, sent in zip(vrefs, out_toks)]\n",
    "# dest_updater = FileParatextProjectTextUpdater(src_file_path.parent)\n",
    "# usfm_out = dest_updater.update_usfm(\n",
    "#     src_file_text.id, rows, strip_all_text=True, prefer_existing_text=False\n",
    "# )\n",
    "# with open(f\"zzz_PN_KTs/{pair}/{book}/{book_name}{file_suffix}_goal.SFM\", \"w\", encoding=src_settings.encoding) as f:\n",
    "#     f.write(usfm_out)\n",
    "with open(src_file_path, encoding=\"utf-8-sig\") as f:\n",
    "    usfm = f.read()\n",
    "handler = ParagraphUpdateUsfmParserHandler(rows, behavior=UpdateUsfmBehavior.PREFER_NEW)\n",
    "parse_usfm(usfm, handler, src_settings.stylesheet, src_settings.versification, preserve_whitespace=False)\n",
    "usfm_out = handler.get_usfm(src_settings.stylesheet)\n",
    "with out_file_path.open(\"w\", encoding=src_settings.encoding) as f:\n",
    "    f.write(usfm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation -- no good\n",
    "* score orig --> no inline markers\n",
    "* score orig --> only para markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from pathlib import Path\n",
    "\n",
    "out = list(load_corpus(Path(\"\")))\n",
    "ref = [list(load_corpus(Path(\"\")))]\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(out, ref, lowercase=True).score\n",
    "spbleu = sacrebleu.corpus_bleu(out, ref, lowercase=True, tokenize=\"flores200\").score\n",
    "chrf = sacrebleu.corpus_chrf(out, ref, char_order=6, beta=3, remove_whitespace=True).score\n",
    "chrfp = sacrebleu.corpus_chrf(out, ref, char_order=6, beta=3, word_order=1, remove_whitespace=True, eps_smoothing=True).score\n",
    "chrfpp = sacrebleu.corpus_chrf(out, ref, char_order=6, beta=3, word_order=2, remove_whitespace=True, eps_smoothing=True).score\n",
    "print(bleu, spbleu, chrf, chrfp, chrfpp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
