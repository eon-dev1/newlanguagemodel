{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to score LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece==0.1.97 # same as silnlp\n",
    "%pip install nltk==3.7 # same as silnlp\n",
    "%pip install sacrebleu==2.3.1 # same as silnlp\n",
    "%pip install rouge_score\n",
    "%pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "from nltk.translate.nist_score import corpus_nist\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"C:/mySIL/preprocessed/\"     # For Windows\n",
    "path = \"/Users/laura/silnlp/scripts/llms/data/preprocessed/\"\n",
    "language = \"xxx\" # fill in language name\n",
    "dataset = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"/\" + \"test_\" + dataset + \"_\" + language + \"_generated.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    trg_predictions = file.readlines()\n",
    "with open(path + language + \"/\" + dataset + \".trg.detok.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    trg = file.readlines()\n",
    "with open(path + language + \"/\" + dataset + \".src.detok.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    src = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: read in src and trg from a json file instead of directly from a txt file\n",
    "\n",
    "import json\n",
    "\n",
    "# Initialize a dictionary to hold the lists for each field\n",
    "dataset_dict = {'input': [], 'output': []}\n",
    "    \n",
    "# Open the file and read line by line\n",
    "with open(\"data/all_llm_data/xxx_smaller_test_data.jsonl\", 'r', encoding='utf-8') as file: # fill in language name\n",
    "  for line in file:\n",
    "    # Each line is a complete JSON object\n",
    "    json_object = json.loads(line)\n",
    "    # Append each field to the appropriate list\n",
    "    instruction = json_object.get('model_inputs', '')[:30]\n",
    "    dataset_dict['input'].append(json_object.get('model_inputs', '')[32:]) #remove prompt from input  \n",
    "    dataset_dict['output'].append(json_object.get('completion', '')[:-2]) #remove \\r\\n from end of output\n",
    "\n",
    "trg = dataset_dict['output']\n",
    "src = dataset_dict['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Group src and trg in ten verse spans\n",
    "trg_grouped = []\n",
    "src_grouped = []\n",
    "\n",
    "starting = 0\n",
    "while starting < len(src):\n",
    "    ending = starting + 10\n",
    "    if ending > len(src):\n",
    "        ending = len(src)\n",
    "    trg_grouped.append(' '.join(trg[starting:ending]))\n",
    "    src_grouped.append(' '.join(src[starting:ending]))\n",
    "    starting = ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: format trg predictions, remove LLM tags\n",
    "trg_predictions = [i.replace(' </VERSE> ',' ').replace('</VERSE>','') for i in trg_predictions]\n",
    "trg_predictions = [i.replace('### Response:','') for i in trg_predictions]\n",
    "trg_predictions = [i.replace('### Input:','') for i in trg_predictions]\n",
    "trg_predictions = [i.replace('### Instruction:','') for i in trg_predictions]\n",
    "trg_predictions = [i.strip() for i in trg_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: just remove <end_of_text|> from LLM output\n",
    "trg_predictions = [i.replace('<|end_of_text|>','').replace('<|endoftext|','').replace('<|endoftext','').strip() + '\\n' for i in trg_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure trg predictions look right\n",
    "trg_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first set of trg prediction, trg, and src\n",
    "print(trg_predictions[0])\n",
    "print(trg[0])\n",
    "print(src[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: automatically remove prompt from trg predictions\n",
    "#trg_predictions = [i[i.find(\":\")+2:] for i in trg_predictions] #32\n",
    "\n",
    "for i in range(len(trg_predictions)):\n",
    "  source_sentence = src[i][:-1] #remove new line character from the end of the source sentence\n",
    "  prediction = trg_predictions[i]\n",
    "  print(source_sentence)\n",
    "  print(prediction)\n",
    "  \n",
    "  if prediction[:len(source_sentence)]==source_sentence: #prediction repeats the source sentence\n",
    "    trg_predictions[i] = prediction[len(source_sentence)+1:] #remove source sentence from prediction\n",
    "    print(trg_predictions[i])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring code\n",
    "pair_sys = trg_predictions\n",
    "pair_refs = [trg]\n",
    "\n",
    "scores = {}\n",
    "\n",
    "bleu_score = sacrebleu.corpus_bleu(pair_sys,pair_refs,lowercase=True,tokenize=\"13a\")\n",
    "scores[\"BLEU\"] = bleu_score.score\n",
    "\n",
    "chrf3_score = sacrebleu.corpus_chrf(pair_sys, pair_refs, char_order=6, beta=3, remove_whitespace=True)\n",
    "scores[\"chrF3\"] = chrf3_score.score\n",
    "\n",
    "chrfp_score = sacrebleu.corpus_chrf(pair_sys, pair_refs, char_order=6, beta=3, word_order=1, remove_whitespace=True, eps_smoothing=True)\n",
    "scores[\"chrF3+\"] = chrfp_score.score\n",
    "\n",
    "chrfpp_score = sacrebleu.corpus_chrf(pair_sys, pair_refs, char_order=6, beta=3, word_order=2, remove_whitespace=True, eps_smoothing=True)\n",
    "scores[\"chrF3++\"] = chrfpp_score.score\n",
    "\n",
    "spbleu_score = sacrebleu.corpus_bleu(pair_sys, pair_refs, lowercase=True,tokenize=\"flores200\",)\n",
    "scores[\"spBLEU\"] = spbleu_score.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure these are all the same len\n",
    "print(len(trg_predictions), len(src), len(trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out scores\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional scoring metrics\n",
    "# METEOR score to evaluate translation quality (synonyms, alignment, stemming, etc.)\n",
    "def compute_meteor(translations, references):\n",
    "    scores = [meteor_score([ref.split()], trans.split()) for trans, ref in zip(translations, references)]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# ROUGE score is sensitive to missing words which is useful for identifying missing words from translations\n",
    "def compute_rouge(translations, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, trans) for trans, ref in zip(translations, references)]\n",
    "    averaged_scores = {key: sum(score[key].fmeasure for score in scores) / len(scores) for key in scores[0]}\n",
    "    return averaged_scores\n",
    "\n",
    "# GLEU score to evaluate sentence level quality\n",
    "def compute_gleu(translations, references):\n",
    "    scores = [sentence_gleu([ref.split()], trans.split()) for trans, ref in zip(translations, references)]\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "# TER score to evalute missing words, segment alignment, etc.\n",
    "def compute_ter(translations, references):\n",
    "    ter_score = sacrebleu.corpus_ter(translations, references).score\n",
    "    return ter_score\n",
    "\n",
    "# Edit Distance to identify misspelled words, punctuation, missing/extra word checking\n",
    "def compute_edit_distance(translations, references):\n",
    "    distances = [editdistance.eval(trans, ref) for trans, ref in zip(translations, references)]\n",
    "    return sum(distances) / len(distances)\n",
    "\n",
    "# precision and recall to measure how well the model captures keywords\n",
    "def compute_precision_recall(translations, references):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for trans, ref in zip(translations, references):\n",
    "        reference_tokens = set(ref.split())\n",
    "        translation_tokens = set(trans.split())\n",
    "        common_tokens = reference_tokens & translation_tokens\n",
    "        precision = len(common_tokens) / len(translation_tokens)\n",
    "        recall = len(common_tokens) / len(reference_tokens)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    return sum(precisions) / len(precisions), sum(recalls) / len(recalls)\n",
    "\n",
    "# NIST score (weighting rare n-grams more heavily)\n",
    "def compute_nist(translations, references):\n",
    "    tokenized_translations = [trans.split() for trans in translations]\n",
    "    tokenized_references = [[ref.split()] for ref in references]\n",
    "    nist_score = corpus_nist(tokenized_references, tokenized_translations, n=5)  # Use n-gram size of 5\n",
    "    return nist_score\n",
    "\n",
    "\n",
    "# Compute scores  \n",
    "#scores[\"METEOR\"] = compute_meteor(pair_sys, pair_refs[0])\n",
    "rouge_scores = compute_rouge(pair_sys, pair_refs[0])\n",
    "for key, value in rouge_scores.items():\n",
    "    scores[f\"ROUGE-{key.upper()}\"] = value\n",
    "scores[\"GLEU\"] = compute_gleu(pair_sys, pair_refs[0])\n",
    "scores[\"TER\"] = compute_ter(pair_sys, pair_refs[0])\n",
    "scores[\"Edit Distance\"] = compute_edit_distance(pair_sys, pair_refs[0])\n",
    "precision, recall = compute_precision_recall(pair_sys, pair_refs[0])\n",
    "scores[\"Precision\"] = precision\n",
    "scores[\"Recall\"] = recall\n",
    "scores[\"NIST\"] = compute_nist(pair_sys, pair_refs[0])\n",
    "\n",
    "# Print the computed scores\n",
    "for score_name, score_value in scores.items():\n",
    "    print(f\"{score_name}: {score_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scores to files\n",
    "\n",
    "import json\n",
    "\n",
    "path = \"/Users/laura/llmResearch/scores/\"\n",
    "\n",
    "def save_to_jsonl(file_path, data, language):\n",
    "\n",
    "    data_with_language_name = {\"language\": language}\n",
    "    data_with_language_name.update(data)\n",
    "    \n",
    "    # Save scores to JSONL file and not overwritting existing rows\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(json.dumps(data_with_language_name) + \"\\n\")\n",
    "\n",
    "data = scores\n",
    "file_path = path + language + \"_\" + dataset + \"_scores.jsonl\"\n",
    "save_to_jsonl(file_path, data, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
