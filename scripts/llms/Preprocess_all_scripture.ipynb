{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm.notebook import tqdm\n",
    "from iso639 import Lang\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/laura/silnlp/silnlp/common\")) # path to silnlp common folder\n",
    "from script_utils import get_script # use get_script function from silnlp\n",
    "\n",
    "path = \"/Users/laura/llmResearch/scripture/\" # path to all Scripture data (downloaded from s3 bucket)\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "print('Before processing - number of Bibles:',len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all bibles, assign language code to each Bible\n",
    "# Takes about 3 minutes\n",
    "\n",
    "bibles = []\n",
    "languages = []\n",
    "filepaths = []\n",
    "num_verses = []\n",
    "scripts = []\n",
    "llm_tags = []\n",
    "\n",
    "# Don't include the languages we want to test on\n",
    "test_languages = ['bft','bcw','bap','ksr','gbj','acw','kje','kfc','kxv','kwf','lmp','mgz','nxq','rjs','kan','tdd','rro','cja']\n",
    "\n",
    "# Cache previously seen iso-3 language codes (speeds it up considerably)\n",
    "language_codes = {}\n",
    "# Exceptions\n",
    "language_codes['pou'] = 'poc'\n",
    "language_codes['sgjj'] = 'sgj' # typo\n",
    "language_codes['in'] = 'ind' # best guess - indonesian\n",
    "language_codes['wra'] = 'wra'\n",
    "language_codes['pltA'] = 'plt'\n",
    "language_codes['pltB'] = 'plt'\n",
    "language_codes['thfL'] = 'thf'\n",
    "language_codes['bapL'] = 'bap'\n",
    "language_codes['dud'] = 'dud'\n",
    "\n",
    "for file_name in tqdm(files):\n",
    "  with open(path + file_name,\"r\",encoding=\"utf-8\") as file:\n",
    "    lines = [i[:-1] for i in file.readlines()] # remove ending newline\n",
    "\n",
    "  if len([i for i in lines if i!='' and i!='...']) == 0: # skip empty bibles\n",
    "    continue\n",
    "\n",
    "  if len(lines) < 31170: # only use full bibles (previous calculations show only about 7% of bibles aren't full)\n",
    "    continue\n",
    "\n",
    "  lines = lines[:31170] # get rid of apocrypha\n",
    "\n",
    "  # Find iso-3 language code for each language\n",
    "  language = file_name[:file_name.find('-')]\n",
    "  if language in language_codes:\n",
    "    iso_code = language_codes[language]\n",
    "  else:\n",
    "    try:\n",
    "      iso_code = Lang(language).pt3\n",
    "    except: # can't find language code\n",
    "      iso_code = ''\n",
    "    language_codes[language] = iso_code\n",
    "\n",
    "  if iso_code not in test_languages:\n",
    "    languages.append(iso_code)\n",
    "    bibles.append(lines)\n",
    "    filepaths.append(file_name)\n",
    "    num_verses.append(len([i for i in lines if i!='' and i!='...']))\n",
    "\n",
    "print('After processing - final number of Bibles:',len(bibles))\n",
    "print('Number of unknown languages:',len([i for i in languages if i==''])) # number of unknown language codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all data in a dataframe to make it easier to sort / manipulate\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['language'] = languages\n",
    "df['filepath'] = filepaths\n",
    "df['num_verses'] = num_verses\n",
    "df['bibles_index'] = range(len(bibles)) # index into bibles list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vref = bibles[list(df[df.filepath=='vref.txt'].bibles_index)[0]]\n",
    "vref_noVerses = [i.split(':')[0] for i in vref] # just leave the book and chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df.filepath=='vref.txt'].index) # just verse names\n",
    "\n",
    "# not real Bible translations\n",
    "df = df.drop(df[df.filepath=='sux-TEST.txt'].index) # not a real Bible translation\n",
    "df = df.drop(df[df.filepath=='ms-MultiCCAligned_id_ms.clean.100K.txt'].index)\n",
    "df = df.drop(df[df.filepath=='id-MultiCCAligned_id_ms.clean.100K.txt'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep correct English pivot translation - remove other English translations\n",
    "keep_index = df[(df.language=='eng') & (df.filepath==\"en-NIV11R.txt\")].index\n",
    "remove_indices = df[(df.language=='eng')].index\n",
    "remove_indices = [i for i in remove_indices if i!=keep_index[0]]\n",
    "df.drop(index=list(remove_indices),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unknown languages\n",
    "df.drop(index=df[df.language==\"\"].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one Bible translation per language (the one with the most verses)\n",
    "print('Before',len(df))\n",
    "df = df.sort_values('num_verses',ascending=False)\n",
    "df = df.drop_duplicates(subset='language',keep='first')\n",
    "print('After',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run if you want to automatically generate script codes for below - otherwise skip\n",
    "# Before running, need to have run previous code to get a list of scripts not_in_dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "iso_scripts = pd.read_csv('data/iso_scripts.csv')\n",
    "\n",
    "for script in set(not_in_dict):\n",
    "  code = list(iso_scripts[iso_scripts['Alias']==script]['Code'])[0]\n",
    "  print(\"script_codes['\"+script.upper()+\"'] = '\" + code + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find script for each Bible we're keeping\n",
    "df = df.reset_index()\n",
    "\n",
    "scripts = []\n",
    "llm_tags = []\n",
    "not_in_dict = []\n",
    "\n",
    "#https://en.wikipedia.org/wiki/ISO_15924\n",
    "# also can generate this automatically using code above\n",
    "script_codes = {}\n",
    "script_codes['CYRILLIC'] = 'Cyrl'\n",
    "script_codes['LATIN'] = 'Latn'\n",
    "script_codes['KANNADA'] = 'Knda'\n",
    "script_codes['GUJARATI'] = 'Gujr'\n",
    "script_codes['ARABIC'] = 'Arab'\n",
    "script_codes['BENGALI'] = 'Beng'\n",
    "script_codes['DEVANAGARI'] = 'Deva'\n",
    "script_codes['ETHIOPIC'] = 'Ethi'\n",
    "script_codes['GEORGIA'] = 'Geor'\n",
    "script_codes['GREEK'] = 'Grek'\n",
    "script_codes['GURMUKHI'] = 'Guru'\n",
    "script_codes['HANGUL'] = 'Hang'\n",
    "script_codes['HEBREW'] = 'Hira'\n",
    "script_codes['KHMER'] = 'Khmr'\n",
    "script_codes['LAO'] = 'Laoo'\n",
    "script_codes['MALAYALAM'] = 'Mlym'\n",
    "script_codes['MYANMAR'] = 'Mymr'\n",
    "script_codes['ORIYA'] = 'Orya'\n",
    "script_codes['SINHALA'] = 'Sinh'\n",
    "script_codes['TAMIL'] = 'Taml'\n",
    "script_codes['TELUGU'] = 'Telu'\n",
    "script_codes['THAI'] = 'Thai'\n",
    "script_codes['TIBETAN'] = 'Tibt'\n",
    "script_codes['VAI'] = 'Vaii'\n",
    "script_codes['TAI_THAM'] = 'Lana'\n",
    "script_codes['TIFINAGH'] = 'Tfng'\n",
    "script_codes['GEORGIAN'] = 'Geor'\n",
    "script_codes['LISU'] = 'Lisu'\n",
    "script_codes['HIRAGANA'] = 'Hira'\n",
    "script_codes['SYRIAC'] = 'Syrc'\n",
    "script_codes['COMMON'] = 'Zyyy'\n",
    "script_codes['MONGOLIAN'] = 'Mong'\n",
    "script_codes['CANADIAN_ABORIGINAL'] = 'Cans'\n",
    "script_codes['KAYAH_LI'] = 'Kali'\n",
    "script_codes['LIMBU'] = 'Limb'\n",
    "script_codes['HAN'] = 'Hani'\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  not_empty = [k for k in bibles[row['bibles_index']] if k != ''] # find non-empty verses\n",
    "  script = get_script(''.join(not_empty[:10])) # look at 10 verses to determine script\n",
    "  if script.upper() in script_codes:\n",
    "    scripts.append(script.capitalize())\n",
    "    llm_tags.append(row['language'] + '_' + script_codes[script.upper()])\n",
    "  else:\n",
    "    not_in_dict.append(script)\n",
    "    scripts.append(script.capitalize())\n",
    "    llm_tags.append(row['language'] + '_Othr')\n",
    "\n",
    "print('scripts not in dictionary',set(not_in_dict))\n",
    "df['script'] = scripts\n",
    "df['llm_tag'] = llm_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_verses = list(df[df.language=='eng'].num_verses)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full bibles: at least 80% of English NIV\n",
    "full_bibles = df[df.num_verses >= 0.8*english_verses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full Bibles so we don't need to re-run everything to this point\n",
    "full_bibles.to_csv(\"data/full_bibles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in full Bibles - if you previously saved them and are picking up here\n",
    "import pandas as pd\n",
    "\n",
    "full_bibles = pd.read_csv(\"data/full_bibles.csv\")\n",
    "full_bibles = full_bibles.drop(columns=[\"Unnamed: 0\",\"index\"])\n",
    "\n",
    "path = \"/Users/laura/llmResearch/scripture/\" # path to all Scripture data (downloaded from s3 bucket)\n",
    "\n",
    "bibles = []\n",
    "for it,row in full_bibles.iterrows():\n",
    "  with open(path + row.filepath,\"r\",encoding=\"utf-8\") as file:\n",
    "    lines = [i[:-1] for i in file.readlines()] # remove ending newline\n",
    "  lines = lines[:31170] # get rid of apocrypha\n",
    "  bibles.append(lines)\n",
    "\n",
    "full_bibles['bibles_index'] = range(len(bibles))\n",
    "\n",
    "# Read in vref\n",
    "with open(path + \"vref.txt\",\"r\",encoding=\"utf-8\") as file:\n",
    "  vref = [i[:-1] for i in file.readlines()] # remove ending newline\n",
    "vref = vref[:31170] # get rid of apocrypha\n",
    "vref_noVerses = [i.split(':')[0] for i in vref] # just leave the book and chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each script are in our set of Bibles?\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for it,row in full_bibles.iterrows():\n",
    "  counter[row.script] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all scripts and counts\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in some of the Ethnologue data\n",
    "ethnologue = pd.read_excel(\"data/Ethnologue/LanguageEthnologAdditionalData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get language family information into full_bibles dataframe\n",
    "full_bibles = pd.merge(full_bibles, ethnologue[[\"LanguageCode\",\"LanguageFamily\"]], left_on=\"language\", right_on=\"LanguageCode\")\n",
    "full_bibles[\"language_family\"] = full_bibles[\"LanguageFamily\"]\n",
    "full_bibles = full_bibles.drop(columns=[\"LanguageCode\",\"LanguageFamily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curate set of languages that we want to work with\n",
    "\n",
    "# For Latin languages, take one Bible from each language family\n",
    "highest_df = full_bibles[full_bibles.script==\"Latin\"].sort_values(\"num_verses\",ascending=False)\n",
    "highest_df = highest_df.drop_duplicates(subset=\"language_family\",keep=\"first\")\n",
    "\n",
    "# Keep all non-Latin languages\n",
    "highest_df = pd.concat([highest_df,full_bibles[full_bibles.script!=\"Latin\"]])\n",
    "\n",
    "# Keep our English pivot translation\n",
    "highest_df = pd.concat([highest_df,full_bibles[full_bibles.language==\"eng\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these languages in case this crashes\n",
    "\n",
    "savepaths = list(highest_df.filepath)\n",
    "\n",
    "import pickle\n",
    "with open('data/103_filepaths.pkl','wb') as pickleFile:\n",
    "  pickle.dump(savepaths,pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using English as a pivot languages, generate inputs, outputs, and instructions for LLM\n",
    "# Takes about 13 minutes\n",
    "\n",
    "pivotLanguage = \"eng\"\n",
    "pivotIndex = list(highest_df[highest_df.language==pivotLanguage].bibles_index)[0]\n",
    "pivotTag = \"eng_Latn\"\n",
    "nonPivotLanguages = list(set(highest_df[highest_df.language != pivotLanguage].language))\n",
    "startingVerse = 0\n",
    "numVerses = 10\n",
    "verseToken = \"</VERSE>\"\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "instructions = []\n",
    "\n",
    "count = 0\n",
    "while startingVerse < 31170:\n",
    "    if count % 100 == 0:\n",
    "        print(startingVerse) # so you can see how fast it's progressing\n",
    "    count += 1\n",
    "\n",
    "    endingVerse = startingVerse + numVerses\n",
    "    if endingVerse > 31170:\n",
    "        endingVerse = 31170\n",
    "    if vref_noVerses[startingVerse] != vref_noVerses[endingVerse-1]:\n",
    "        for i in range(startingVerse+1,endingVerse):\n",
    "            if vref_noVerses[i] != vref_noVerses[startingVerse]:\n",
    "                endingVerse = i\n",
    "                break\n",
    "    \n",
    "    pivotRelevant = bibles[pivotIndex][startingVerse:endingVerse]\n",
    "\n",
    "    for language in nonPivotLanguages:\n",
    "        relevant = bibles[list(highest_df[(highest_df.language==language)].bibles_index)[0]][startingVerse:endingVerse]\n",
    "        translation = (' ' + verseToken + ' ').join(relevant) + ' ' + verseToken\n",
    "        if translation.strip() == \"\": # no verses here\n",
    "            continue\n",
    "\n",
    "        # only include verses in target translation\n",
    "        pivotTranslation = (' ' + verseToken + ' ').join([pivotRelevant[i] for i in range(len(pivotRelevant)) if relevant[i].strip()!='']) + ' ' + verseToken\n",
    "        if pivotTranslation.strip() == \"\": # no verses here\n",
    "            continue\n",
    "\n",
    "        inputs.append(pivotTranslation)\n",
    "        outputs.append(translation)\n",
    "        instructions.append(\"Translate from \" + pivotTag + \" to \" + list(highest_df[(highest_df.language==language)].llm_tag)[0])\n",
    "\n",
    "    startingVerse = endingVerse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LLM data as a JSON file\n",
    "# Takes less than a minute\n",
    "\n",
    "import json\n",
    "\n",
    "# If file already exists, this will add on to the file, not erase what's already in the file - watch out for this behavior\n",
    "with open('data/103languages.jsonl', 'w') as output_file:\n",
    "  for input, output, instruction in zip(inputs, outputs, instructions):\n",
    "      data = {\n",
    "          \"input\": input.strip(),\n",
    "          \"output\": output.strip(),\n",
    "          \"instruction\": instruction.strip()\n",
    "      }\n",
    "      json.dump(data, output_file)\n",
    "      output_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which tokens to add to the tokenizers\n",
    "tokens = set()\n",
    "for it,row in highest_df.iterrows():\n",
    "  tokens.add(row.language+'_')\n",
    "  tokens.add(row.llm_tag[4:]) # script abbreviation\n",
    "\n",
    "print(list(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
